{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d68edb",
   "metadata": {},
   "source": [
    "# ELM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965bfe80",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b150258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6af6f0",
   "metadata": {},
   "source": [
    "# Dateset Class and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c991e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELM_Dataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        super(ELM_Dataset, self).__init__()\n",
    "\n",
    "        self.features = torch.Tensor(features, dtype = torch.float32)\n",
    "        self.labels = torch.Tensor(labels, dtype = torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d92cfe",
   "metadata": {},
   "source": [
    "## ELM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8771fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELM(nn.Module):\n",
    "    def __init__(self, n_hidden_neurons, device = 'cuda', chunk_size = 100):\n",
    "        super(ELM, self).__init__()\n",
    "\n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        self.device = device\n",
    "        self.chunk_size = chunk_size\n",
    "        self.input_weights = None\n",
    "        self.biases = None\n",
    "        self.output_weights = None\n",
    "\n",
    "    def fit(self, X, y, regularization = 1e-8):\n",
    "        \n",
    "        X = X.to(self.device)\n",
    "        y = y.to(self.device).view(-1, 1)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.input_weights = torch.randn(n_features, self.n_hidden_neurons, device = self.device, dtype = torch.float32)\n",
    "        self.biases = torch.randn(1, self.n_hidden_neurons, device = self.device, dtype = torch.float32)\n",
    "\n",
    "        # Check if we need micro-batching based on memory requirements\n",
    "        if torch.cuda.is_available():\n",
    "            available_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            required_mem = n_samples * self.n_hidden_neurons * 4 / 1e9  # 4 bytes per float32\n",
    "            \n",
    "            if required_mem > available_mem * 0.6:  # Use micro-batching if H matrix > 60% of GPU memory\n",
    "                print(f\"Large matrix detected ({required_mem:.1f} GB). Using micro-batching...\")\n",
    "                return self._fit_micro_batched(X, y, regularization)\n",
    "        \n",
    "        # Standard ELM computation\n",
    "        try:\n",
    "            H = torch.sigmoid(X @ self.input_weights + self.biases)\n",
    "            HTH = H.T @ H\n",
    "            HTy = H.T @ y\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(\"GPU out of memory. Switching to micro-batching...\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return self._fit_micro_batched(X, y, regularization)\n",
    "\n",
    "        I = torch.eye(self.n_hidden_neurons, device=self.device)\n",
    "        self.output_weights = torch.linalg.solve(HTH + regularization * I, HTy)\n",
    "\n",
    "    def _fit_micro_batched(self, X, y, regularization):\n",
    "        \"\"\"Micro-batching to handle large matrices\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Initialize accumulator matrices\n",
    "        HTH = torch.zeros(self.n_hidden_neurons, self.n_hidden_neurons, device=self.device, dtype=torch.float32)\n",
    "        HTy = torch.zeros(self.n_hidden_neurons, 1, device=self.device, dtype=torch.float32)\n",
    "        \n",
    "        # Process in small chunks\n",
    "        for i in range(0, n_samples, self.chunk_size):\n",
    "            end_idx = min(i + self.chunk_size, n_samples)\n",
    "            X_chunk = X[i:end_idx]\n",
    "            y_chunk = y[i:end_idx]\n",
    "            \n",
    "            # Compute hidden layer for chunk\n",
    "            H_chunk = torch.sigmoid(X_chunk @ self.input_weights + self.biases)\n",
    "            \n",
    "            # Accumulate HTH and HTy\n",
    "            HTH += H_chunk.T @ H_chunk\n",
    "            HTy += H_chunk.T @ y_chunk\n",
    "            \n",
    "            # Clear chunk from memory\n",
    "            del H_chunk\n",
    "            \n",
    "        # Solve for output weights using accumulated matrices\n",
    "        I = torch.eye(self.n_hidden_neurons, device=self.device)\n",
    "        self.output_weights = torch.linalg.solve(HTH + regularization * I, HTy)\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        X = X.to(self.device)\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Use micro-batching for prediction if needed\n",
    "        if n_samples > self.chunk_size * 2:  # Use chunking for large prediction sets\n",
    "            predictions_list = []\n",
    "            \n",
    "            for i in range(0, n_samples, self.chunk_size):\n",
    "                end_idx = min(i + self.chunk_size, n_samples)\n",
    "                X_chunk = X[i:end_idx]\n",
    "                \n",
    "                H_chunk = torch.sigmoid(X_chunk @ self.input_weights + self.biases)\n",
    "                pred_chunk = H_chunk @ self.output_weights\n",
    "                predictions_list.append((pred_chunk > 0.5).int().flatten())\n",
    "                \n",
    "                del H_chunk, pred_chunk\n",
    "            \n",
    "            return torch.cat(predictions_list, dim=0)\n",
    "        \n",
    "        # Standard prediction\n",
    "        H = torch.sigmoid(X @ self.input_weights + self.biases)\n",
    "        predictions = H @ self.output_weights\n",
    "\n",
    "        return (predictions > 0.5).int().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c38937db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Loading and preparing MEFAR_MID.csv data...\")\n",
    "# df = pd.read_csv('../Datasets/MEFAR_MID.csv')\n",
    "\n",
    "# # Handle missing values\n",
    "# if df.isnull().sum().sum() > 0:\n",
    "#     df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# X = df.iloc[:, :-1].values\n",
    "# y = df.iloc[:, -1].values\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# X_train_pt = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "# y_train_pt = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_test_pt = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# # --- 2. Set up the Tuning Loop ---\n",
    "# # Define the range of hidden neuron values to test\n",
    "# neuron_counts = [3000, 3500, 4000, 5000, 6000, 7000, 8000, 10000]\n",
    "# accuracies = []\n",
    "\n",
    "# print(f\"\\nStarting hyperparameter tuning on {device}...\")\n",
    "\n",
    "# for n_neurons in neuron_counts:\n",
    "#     # Train the model with the current number of neurons\n",
    "#     model = ELM(n_hidden_neurons=n_neurons)\n",
    "#     model.fit(X_train_pt, y_train_pt)\n",
    "    \n",
    "#     # Evaluate the model\n",
    "#     predictions_pt = model.predict(X_test_pt)\n",
    "#     predictions_np = predictions_pt.cpu().numpy()\n",
    "    \n",
    "#     acc = accuracy_score(y_test, predictions_np)\n",
    "#     accuracies.append(acc)\n",
    "    \n",
    "#     print(f\"Neurons = {n_neurons}, Accuracy = {acc * 100:.2f}%\")\n",
    "\n",
    "# # --- 3. Find and Print the Best Result ---\n",
    "# best_accuracy = max(accuracies)\n",
    "# best_n_neurons = neuron_counts[accuracies.index(best_accuracy)]\n",
    "\n",
    "# print(f\"\\nTuning Complete!\")\n",
    "# print(f\"Best Accuracy: {best_accuracy * 100:.2f}% achieved with {best_n_neurons} hidden neurons.\")\n",
    "\n",
    "# # --- 4. Plot the Results ---\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(neuron_counts, accuracies, marker='o', linestyle='-')\n",
    "# plt.title('ELM Performance vs. Number of Hidden Neurons')\n",
    "# plt.xlabel('Number of Hidden Neurons')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xticks(neuron_counts)\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ade231",
   "metadata": {},
   "source": [
    "## Initialization and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f604b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Available GPU memory: 8.6 GB\n",
      "Processing MEFAR_DOWN.csv\n",
      "Found missing values. Filling with column mean.\n",
      "Accuracy for MEFAR_DOWN.csv: 51.18%\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.52      0.51      2673\n",
      "         1.0       0.53      0.51      0.52      2841\n",
      "\n",
      "    accuracy                           0.51      5514\n",
      "   macro avg       0.51      0.51      0.51      5514\n",
      "weighted avg       0.51      0.51      0.51      5514\n",
      "\n",
      "-------------------------------------------\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Available GPU memory: 8.6 GB\n",
      "Processing MEFAR_MID.csv\n",
      "Large matrix detected (59.1 GB). Using micro-batching...\n",
      "Large matrix detected (59.1 GB). Using micro-batching...\n",
      "Accuracy for MEFAR_MID.csv: 94.57%\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.95      0.95     92491\n",
      "         1.0       0.95      0.94      0.95     92169\n",
      "\n",
      "    accuracy                           0.95    184660\n",
      "   macro avg       0.95      0.95      0.95    184660\n",
      "weighted avg       0.95      0.95      0.95    184660\n",
      "\n",
      "-------------------------------------------\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Available GPU memory: 8.6 GB\n",
      "Processing MEFAR_UP.csv\n",
      "Large matrix detected (118.2 GB). Using micro-batching...\n",
      "Large matrix detected (118.2 GB). Using micro-batching...\n",
      "Accuracy for MEFAR_UP.csv: 94.01%\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.94      0.94    184829\n",
      "         1.0       0.94      0.94      0.94    184489\n",
      "\n",
      "    accuracy                           0.94    369318\n",
      "   macro avg       0.94      0.94      0.94    369318\n",
      "weighted avg       0.94      0.94      0.94    369318\n",
      "\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_NEURONS = 20000\n",
    "DATASET_PATH = '../Datasets'\n",
    "\n",
    "dataset_list = os.listdir(path = DATASET_PATH)\n",
    "\n",
    "for filename in dataset_list:\n",
    "\n",
    "    #Checking for GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "\n",
    "    #Processing each Dataset\n",
    "    print(f'Processing {filename}')\n",
    "\n",
    "    file_path = f'../Datasets/{filename}'\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    #Checking and Handling Missing Values\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        print(\"Found missing values. Filling with column mean.\")\n",
    "        df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_train_pt = torch.tensor(X_train_scaled, dtype = torch.float32)\n",
    "    y_train_pt = torch.tensor(y_train, dtype = torch.float32)\n",
    "    X_test_pt = torch.tensor(X_test_scaled, dtype = torch.float32)\n",
    "\n",
    "    model = ELM(n_hidden_neurons = HIDDEN_NEURONS, device = device)\n",
    "\n",
    "    # Checking Training Time\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_pt, y_train_pt)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    predictions = model.predict(X_test_pt)\n",
    "    predictions_np = predictions.cpu().numpy()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions_np)\n",
    "\n",
    "    report = classification_report(y_test, predictions_np, zero_division = 0)\n",
    "\n",
    "    print(f'Accuracy for {filename}: {accuracy * 100:.2f}%\\n')\n",
    "    print('Classification Report')\n",
    "    print(report)\n",
    "    print('-------------------------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "963b741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hidden_neurons(X_train, y_train, X_test, y_test, neuron_range=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    Tune the number of hidden neurons for base ELM\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv('../Datasets/MEFAR_MID.csv')\n",
    "\n",
    "    # Handle missing values\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_train_pt = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    y_train_pt = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test_pt = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "    if neuron_range is None:\n",
    "        neuron_range = [10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000]\n",
    "    \n",
    "    X_train_pt = X_train_pt.to(device)\n",
    "    y_train_pt = y_train_pt.to(device)\n",
    "    X_test_pt = X_test_pt.to(device)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Tuning number of hidden neurons...\")\n",
    "    for n_neurons in neuron_range:\n",
    "        model = ELM(n_hidden_neurons=n_neurons, device=device, chunk_size=100)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model.fit(X_train_pt, y_train_pt)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        predictions = model.predict(X_test_pt)\n",
    "        accuracy = accuracy_score(y_test, predictions.cpu().numpy())\n",
    "        \n",
    "        results.append({\n",
    "            'neurons': n_neurons,\n",
    "            'accuracy': accuracy,\n",
    "            'training_time': training_time\n",
    "        })\n",
    "        \n",
    "        print(f\"Neurons: {n_neurons:5d} | Accuracy: {accuracy*100:6.2f}% | Time: {training_time:.3f}s\")\n",
    "        \n",
    "        # Clear cache after each test\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Find best configuration\n",
    "    best_result = max(results, key=lambda x: x['accuracy'])\n",
    "    print(f\"\\nBest configuration:\")\n",
    "    print(f\"Neurons: {best_result['neurons']}\")\n",
    "    print(f\"Accuracy: {best_result['accuracy']*100:.2f}%\")\n",
    "    print(f\"Training time: {best_result['training_time']:.3f}s\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c983dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning number of hidden neurons...\n",
      "Large matrix detected (29.5 GB). Using micro-batching...\n",
      "Neurons: 10000 | Accuracy:  88.54% | Time: 52.458s\n",
      "Large matrix detected (32.5 GB). Using micro-batching...\n",
      "Neurons: 11000 | Accuracy:  89.33% | Time: 63.200s\n",
      "Large matrix detected (35.5 GB). Using micro-batching...\n",
      "Neurons: 12000 | Accuracy:  89.79% | Time: 75.134s\n",
      "Large matrix detected (38.4 GB). Using micro-batching...\n",
      "Neurons: 13000 | Accuracy:  90.66% | Time: 88.286s\n",
      "Large matrix detected (41.4 GB). Using micro-batching...\n",
      "Neurons: 14000 | Accuracy:  91.41% | Time: 102.376s\n",
      "Large matrix detected (44.3 GB). Using micro-batching...\n",
      "Neurons: 15000 | Accuracy:  91.79% | Time: 117.471s\n",
      "Large matrix detected (47.3 GB). Using micro-batching...\n",
      "Neurons: 16000 | Accuracy:  92.52% | Time: 133.515s\n",
      "Large matrix detected (50.2 GB). Using micro-batching...\n",
      "Neurons: 17000 | Accuracy:  93.07% | Time: 150.764s\n",
      "Large matrix detected (53.2 GB). Using micro-batching...\n",
      "Neurons: 18000 | Accuracy:  93.64% | Time: 169.103s\n",
      "Large matrix detected (56.1 GB). Using micro-batching...\n",
      "Neurons: 19000 | Accuracy:  94.09% | Time: 189.153s\n",
      "Large matrix detected (59.1 GB). Using micro-batching...\n",
      "Neurons: 20000 | Accuracy:  94.64% | Time: 209.776s\n",
      "\n",
      "Best configuration:\n",
      "Neurons: 20000\n",
      "Accuracy: 94.64%\n",
      "Training time: 209.776s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'neurons': 10000,\n",
       "  'accuracy': 0.885421856384707,\n",
       "  'training_time': 52.45798134803772},\n",
       " {'neurons': 11000,\n",
       "  'accuracy': 0.8932578793458248,\n",
       "  'training_time': 63.19957518577576},\n",
       " {'neurons': 12000,\n",
       "  'accuracy': 0.8979205025452183,\n",
       "  'training_time': 75.13354444503784},\n",
       " {'neurons': 13000,\n",
       "  'accuracy': 0.9065579984836998,\n",
       "  'training_time': 88.28577899932861},\n",
       " {'neurons': 14000,\n",
       "  'accuracy': 0.9141124228311491,\n",
       "  'training_time': 102.37623238563538},\n",
       " {'neurons': 15000,\n",
       "  'accuracy': 0.9178598505361204,\n",
       "  'training_time': 117.4709951877594},\n",
       " {'neurons': 16000,\n",
       "  'accuracy': 0.9251976605653633,\n",
       "  'training_time': 133.5154423713684},\n",
       " {'neurons': 17000,\n",
       "  'accuracy': 0.9306671721000758,\n",
       "  'training_time': 150.76416730880737},\n",
       " {'neurons': 18000,\n",
       "  'accuracy': 0.9364236976064118,\n",
       "  'training_time': 169.10250687599182},\n",
       " {'neurons': 19000,\n",
       "  'accuracy': 0.9408805372035092,\n",
       "  'training_time': 189.15259957313538},\n",
       " {'neurons': 20000,\n",
       "  'accuracy': 0.9463933716018629,\n",
       "  'training_time': 209.7762806415558}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_hidden_neurons(X_train, y_train, X_test, y_test, neuron_range=None, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = max(results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nBest configuration:\")\n",
    "print(f\"Neurons: {best_result['neurons']}\")\n",
    "print(f\"Accuracy: {best_result['accuracy']*100:.2f}%\")\n",
    "print(f\"Training time: {best_result['training_time']:.3f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
