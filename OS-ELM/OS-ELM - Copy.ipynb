{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10575770",
   "metadata": {},
   "source": [
    "# OS - ELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ade9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 3 dataset files\n",
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE OS-ELM TUNING FOR MEFAR_DOWN.csv\n",
      "======================================================================\n",
      "Dataset loaded: (27570, 18)\n",
      "Handling missing values...\n",
      "Features: 17, Samples: 27570\n",
      "Classes: [0. 1.], Distribution: [13106 14464]\n",
      "Training samples: 22056, Test samples: 5514\n",
      "\n",
      "==================================================\n",
      "Configuration 1/10\n",
      "Config: {'n_hidden': 2500, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 1}\n",
      "==================================================\n",
      "Training strategy: 2500 initial, 19556 sequential\n",
      "Micro-batch size: 1\n",
      "Initial training: 2500 samples, 17 features, 2500 neurons\n",
      "Hidden layer shape: torch.Size([2500, 2500]), Target shape: torch.Size([2500, 1])\n",
      "Initial training successful. Regularization: 0.010000\n",
      "Output weights shape: torch.Size([2500, 1])\n",
      "Sequential training: 19556 individual samples\n",
      "Results:\n",
      "  Train Accuracy: 47.39%\n",
      "  Test Accuracy:  47.61%\n",
      "  Training Time:  19.795s\n",
      "  Overfitting:    -0.22%\n",
      "\n",
      "==================================================\n",
      "Configuration 2/10\n",
      "Config: {'n_hidden': 2500, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 5}\n",
      "==================================================\n",
      "Training strategy: 2500 initial, 19556 sequential\n",
      "Micro-batch size: 5\n",
      "Initial training: 2500 samples, 17 features, 2500 neurons\n",
      "Hidden layer shape: torch.Size([2500, 2500]), Target shape: torch.Size([2500, 1])\n",
      "Initial training successful. Regularization: 0.010000\n",
      "Output weights shape: torch.Size([2500, 1])\n",
      "Sequential training: 3912 micro-batches of size ≤5\n",
      "Results:\n",
      "  Train Accuracy: 81.97%\n",
      "  Test Accuracy:  74.90%\n",
      "  Training Time:  4.508s\n",
      "  Overfitting:    7.07%\n",
      "\n",
      "==================================================\n",
      "Configuration 3/10\n",
      "Config: {'n_hidden': 2500, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 10}\n",
      "==================================================\n",
      "Training strategy: 2500 initial, 19556 sequential\n",
      "Micro-batch size: 10\n",
      "Initial training: 2500 samples, 17 features, 2500 neurons\n",
      "Hidden layer shape: torch.Size([2500, 2500]), Target shape: torch.Size([2500, 1])\n",
      "Initial training successful. Regularization: 0.010000\n",
      "Output weights shape: torch.Size([2500, 1])\n",
      "Sequential training: 1956 micro-batches of size ≤10\n",
      "Results:\n",
      "  Train Accuracy: 81.96%\n",
      "  Test Accuracy:  74.92%\n",
      "  Training Time:  2.304s\n",
      "  Overfitting:    7.04%\n",
      "\n",
      "==================================================\n",
      "Configuration 4/10\n",
      "Config: {'n_hidden': 2500, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 20}\n",
      "==================================================\n",
      "Training strategy: 2500 initial, 19556 sequential\n",
      "Micro-batch size: 20\n",
      "Initial training: 2500 samples, 17 features, 2500 neurons\n",
      "Hidden layer shape: torch.Size([2500, 2500]), Target shape: torch.Size([2500, 1])\n",
      "Initial training successful. Regularization: 0.010000\n",
      "Output weights shape: torch.Size([2500, 1])\n",
      "Sequential training: 978 micro-batches of size ≤20\n",
      "Results:\n",
      "  Train Accuracy: 81.96%\n",
      "  Test Accuracy:  74.88%\n",
      "  Training Time:  1.196s\n",
      "  Overfitting:    7.07%\n",
      "\n",
      "==================================================\n",
      "Configuration 5/10\n",
      "Config: {'n_hidden': 5000, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 10}\n",
      "==================================================\n",
      "Training strategy: 5000 initial, 17056 sequential\n",
      "Micro-batch size: 10\n",
      "Initial training: 5000 samples, 17 features, 5000 neurons\n",
      "Hidden layer shape: torch.Size([5000, 5000]), Target shape: torch.Size([5000, 1])\n",
      "Initial training successful. Regularization: 0.010000\n",
      "Output weights shape: torch.Size([5000, 1])\n",
      "Sequential training: 1706 micro-batches of size ≤10\n",
      "Results:\n",
      "  Train Accuracy: 66.18%\n",
      "  Test Accuracy:  62.77%\n",
      "  Training Time:  6.538s\n",
      "  Overfitting:    3.41%\n",
      "\n",
      "==================================================\n",
      "Configuration 6/10\n",
      "Config: {'n_hidden': 7500, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 10}\n",
      "==================================================\n",
      "Training strategy: 7500 initial, 14556 sequential\n",
      "Micro-batch size: 10\n",
      "Initial training: 7500 samples, 17 features, 7500 neurons\n",
      "Hidden layer shape: torch.Size([7500, 7500]), Target shape: torch.Size([7500, 1])\n",
      "Initial training successful. Regularization: 0.010000\n",
      "Output weights shape: torch.Size([7500, 1])\n",
      "Sequential training: 1456 micro-batches of size ≤10\n",
      "Results:\n",
      "  Train Accuracy: 51.83%\n",
      "  Test Accuracy:  50.60%\n",
      "  Training Time:  12.262s\n",
      "  Overfitting:    1.23%\n",
      "\n",
      "==================================================\n",
      "Configuration 7/10\n",
      "Config: {'n_hidden': 10000, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 10}\n",
      "==================================================\n",
      "Training strategy: 10000 initial, 12056 sequential\n",
      "Micro-batch size: 10\n",
      "Initial training: 10000 samples, 17 features, 10000 neurons\n",
      "Hidden layer shape: torch.Size([10000, 10000]), Target shape: torch.Size([10000, 1])\n",
      "Initial training successful. Regularization: 0.010000\n",
      "Output weights shape: torch.Size([10000, 1])\n",
      "Sequential training: 1206 micro-batches of size ≤10\n",
      "Results:\n",
      "  Train Accuracy: 52.10%\n",
      "  Test Accuracy:  50.96%\n",
      "  Training Time:  17.653s\n",
      "  Overfitting:    1.14%\n",
      "\n",
      "==================================================\n",
      "Configuration 8/10\n",
      "Config: {'n_hidden': 5000, 'activation': 'tanh', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 10}\n",
      "==================================================\n",
      "Training strategy: 5000 initial, 17056 sequential\n",
      "Micro-batch size: 10\n",
      "Initial training: 5000 samples, 17 features, 5000 neurons\n",
      "Hidden layer shape: torch.Size([5000, 5000]), Target shape: torch.Size([5000, 1])\n",
      "Initial training successful. Regularization: 0.010000\n",
      "Output weights shape: torch.Size([5000, 1])\n",
      "Sequential training: 1706 micro-batches of size ≤10\n",
      "Results:\n",
      "  Train Accuracy: 48.05%\n",
      "  Test Accuracy:  47.24%\n",
      "  Training Time:  6.409s\n",
      "  Overfitting:    0.80%\n",
      "\n",
      "==================================================\n",
      "Configuration 9/10\n",
      "Config: {'n_hidden': 5000, 'activation': 'relu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 10}\n",
      "==================================================\n",
      "Training strategy: 5000 initial, 17056 sequential\n",
      "Micro-batch size: 10\n",
      "Initial training: 5000 samples, 17 features, 5000 neurons\n",
      "Hidden layer shape: torch.Size([5000, 5000]), Target shape: torch.Size([5000, 1])\n",
      "Initial training successful. Regularization: 0.010000\n",
      "Output weights shape: torch.Size([5000, 1])\n",
      "Sequential training: 1706 micro-batches of size ≤10\n",
      "Results:\n",
      "  Train Accuracy: 77.28%\n",
      "  Test Accuracy:  68.63%\n",
      "  Training Time:  6.461s\n",
      "  Overfitting:    8.66%\n",
      "\n",
      "==================================================\n",
      "Configuration 10/10\n",
      "Config: {'n_hidden': 5000, 'activation': 'swish', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 10}\n",
      "==================================================\n",
      "Training strategy: 5000 initial, 17056 sequential\n",
      "Micro-batch size: 10\n",
      "Initial training: 5000 samples, 17 features, 5000 neurons\n",
      "Hidden layer shape: torch.Size([5000, 5000]), Target shape: torch.Size([5000, 1])\n",
      "Initial training successful. Regularization: 0.010000\n",
      "Output weights shape: torch.Size([5000, 1])\n",
      "Sequential training: 1706 micro-batches of size ≤10\n",
      "Results:\n",
      "  Train Accuracy: 66.06%\n",
      "  Test Accuracy:  62.59%\n",
      "  Training Time:  6.375s\n",
      "  Overfitting:    3.47%\n",
      "\n",
      "==================================================\n",
      "ANALYSIS SUMMARY\n",
      "==================================================\n",
      "Best Configuration:\n",
      "  Hidden Neurons: 2500\n",
      "  Activation: gelu\n",
      "  Regularization: 0.1000\n",
      "  Micro-batch Size: 10\n",
      "  Test Accuracy: 74.92%\n",
      "  Training Time: 2.304s\n",
      "\n",
      "Performance by Micro-batch Size:\n",
      "  Batch size 1: 47.61% accuracy, 19.795s avg time\n",
      "  Batch size 5: 74.90% accuracy, 4.508s avg time\n",
      "  Batch size 10: 59.67% accuracy, 8.286s avg time\n",
      "  Batch size 20: 74.88% accuracy, 1.196s avg time\n",
      "\n",
      "Performance by Activation Function:\n",
      "  gelu: 62.38% ± 12.64%\n",
      "  tanh: 47.24% ± nan%\n",
      "  relu: 68.63% ± nan%\n",
      "  swish: 62.59% ± nan%\n",
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE OS-ELM TUNING FOR MEFAR_MID.csv\n",
      "======================================================================\n",
      "Dataset loaded: (923298, 18)\n",
      "Features: 17, Samples: 923298\n",
      "Classes: [0. 1.], Distribution: [461649 461649]\n",
      "Training samples: 738638, Test samples: 184660\n",
      "\n",
      "==================================================\n",
      "Configuration 1/10\n",
      "Config: {'n_hidden': 2500, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 1}\n",
      "==================================================\n",
      "Training strategy: 73863 initial, 664775 sequential\n",
      "Micro-batch size: 1\n",
      "Initial training: 73863 samples, 17 features, 2500 neurons\n",
      "Hidden layer shape: torch.Size([73863, 2500]), Target shape: torch.Size([73863, 1])\n",
      "Initial training successful. Regularization: 0.010000\n",
      "Output weights shape: torch.Size([2500, 1])\n",
      "Sequential training: 664775 individual samples\n",
      "Results:\n",
      "  Train Accuracy: 80.60%\n",
      "  Test Accuracy:  80.52%\n",
      "  Training Time:  791.013s\n",
      "  Overfitting:    0.08%\n",
      "\n",
      "==================================================\n",
      "Configuration 2/10\n",
      "Config: {'n_hidden': 2500, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 5}\n",
      "==================================================\n",
      "Training strategy: 73863 initial, 664775 sequential\n",
      "Micro-batch size: 5\n",
      "Initial training: 73863 samples, 17 features, 2500 neurons\n",
      "Hidden layer shape: torch.Size([73863, 2500]), Target shape: torch.Size([73863, 1])\n",
      "Initial training successful. Regularization: 0.010000\n",
      "Output weights shape: torch.Size([2500, 1])\n",
      "Sequential training: 132955 micro-batches of size ≤5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 609\u001b[0m\n\u001b[0;32m    607\u001b[0m all_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m dataset_files:\n\u001b[1;32m--> 609\u001b[0m     results \u001b[38;5;241m=\u001b[39m comprehensive_elm_tuning(file, test_configurations, device)\n\u001b[0;32m    610\u001b[0m     all_results[file] \u001b[38;5;241m=\u001b[39m results\n\u001b[0;32m    612\u001b[0m \u001b[38;5;66;03m# Overall summary\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 481\u001b[0m, in \u001b[0;36mcomprehensive_elm_tuning\u001b[1;34m(file_path, test_configurations, device)\u001b[0m\n\u001b[0;32m    478\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[1;32m--> 481\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(X_train_scaled, y_train)\n\u001b[0;32m    482\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(X_test_scaled, y_test)\n\u001b[0;32m    484\u001b[0m result \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_hidden\u001b[39m\u001b[38;5;124m'\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_hidden\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m: config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverfitting\u001b[39m\u001b[38;5;124m'\u001b[39m: train_accuracy \u001b[38;5;241m-\u001b[39m test_accuracy\n\u001b[0;32m    495\u001b[0m }\n",
      "Cell \u001b[1;32mIn[1], line 409\u001b[0m, in \u001b[0;36mRobustOSELM.score\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute accuracy score.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(X))\n",
      "Cell \u001b[1;32mIn[1], line 403\u001b[0m, in \u001b[0;36mRobustOSELM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    402\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict class labels.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    404\u001b[0m     predicted_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[predicted_indices]\n",
      "Cell \u001b[1;32mIn[1], line 389\u001b[0m, in \u001b[0;36mRobustOSELM.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    386\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_hidden_layer(X)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# Compute raw predictions\u001b[39;00m\n\u001b[1;32m--> 389\u001b[0m raw_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(H, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_weights)  \u001b[38;5;66;03m# (n_samples, n_outputs)\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;66;03m# Binary classification with sigmoid\u001b[39;00m\n\u001b[0;32m    393\u001b[0m     probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(raw_output)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Union, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class RobustOSELM(nn.Module):\n",
    "    \"\"\"\n",
    "    Online Sequential Extreme Learning Machine with micro-batching support.\n",
    "    \n",
    "    Features:\n",
    "    - Micro-batching for computational efficiency\n",
    "    - Proper hyperparameter ranges\n",
    "    - Numerical stability improvements\n",
    "    - Adaptive regularization\n",
    "    - Better initialization strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_hidden: int = 200,\n",
    "                 activation: str = 'tanh',\n",
    "                 regularization: float = 0.1,\n",
    "                 device: str = 'auto',\n",
    "                 random_state: Optional[int] = None,\n",
    "                 adaptive_reg: bool = True,\n",
    "                 micro_batch_size: int = 1):\n",
    "        super(RobustOSELM, self).__init__()\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "        self.activation = activation\n",
    "        self.regularization = regularization\n",
    "        self.adaptive_reg = adaptive_reg\n",
    "        self.random_state = random_state\n",
    "        self.micro_batch_size = micro_batch_size\n",
    "        \n",
    "        # Set device\n",
    "        if device == 'auto':\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        # Model parameters\n",
    "        self.input_weights = None\n",
    "        self.biases = None\n",
    "        self.output_weights = None\n",
    "        self.P_matrix = None  # Precision matrix for online updates\n",
    "        self.n_features = None\n",
    "        self.n_classes = None\n",
    "        self.classes_ = None\n",
    "        self.is_fitted = False\n",
    "        self.sample_count = 0\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        if random_state is not None:\n",
    "            torch.manual_seed(random_state)\n",
    "            np.random.seed(random_state)\n",
    "    \n",
    "    def _get_activation_function(self):\n",
    "        \"\"\"Get activation function with improved options.\"\"\"\n",
    "        if self.activation == 'sigmoid':\n",
    "            return lambda x: torch.sigmoid(x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return lambda x: torch.tanh(x)\n",
    "        elif self.activation == 'relu':\n",
    "            return lambda x: torch.relu(x)\n",
    "        elif self.activation == 'leaky_relu':\n",
    "            return lambda x: torch.leaky_relu(x, 0.01)\n",
    "        elif self.activation == 'swish':\n",
    "            return lambda x: x * torch.sigmoid(x)\n",
    "        elif self.activation == 'gelu':\n",
    "            return lambda x: torch.nn.functional.gelu(x)\n",
    "        else:\n",
    "            return lambda x: torch.tanh(x)  # Default fallback\n",
    "    \n",
    "    def _initialize_network(self, n_features):\n",
    "        \"\"\"Initialize network with proper scaling.\"\"\"\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # Xavier/Glorot initialization for better convergence\n",
    "        if self.activation in ['sigmoid', 'tanh']:\n",
    "            # Xavier uniform initialization\n",
    "            limit = np.sqrt(6.0 / (n_features + self.n_hidden))\n",
    "            self.input_weights = (torch.rand(self.n_hidden, n_features, device=self.device) * 2 * limit) - limit\n",
    "        else:\n",
    "            # He initialization for ReLU-like activations\n",
    "            std = np.sqrt(2.0 / n_features)\n",
    "            self.input_weights = torch.randn(self.n_hidden, n_features, device=self.device) * std\n",
    "        \n",
    "        # Initialize biases\n",
    "        self.biases = (torch.rand(self.n_hidden, 1, device=self.device) * 2) - 1\n",
    "    \n",
    "    def _compute_hidden_layer(self, X):\n",
    "        \"\"\"Compute hidden layer activations with numerical stability.\"\"\"\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # Compute pre-activation: X @ W^T + b\n",
    "        linear = torch.mm(X, self.input_weights.T) + self.biases.T\n",
    "        \n",
    "        # Apply activation with numerical clipping for stability\n",
    "        activation_fn = self._get_activation_function()\n",
    "        \n",
    "        # Clip extreme values to prevent overflow\n",
    "        linear = torch.clamp(linear, min=-50, max=50)\n",
    "        hidden = activation_fn(linear)\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "    def _encode_targets(self, y):\n",
    "        \"\"\"Encode target values appropriately.\"\"\"\n",
    "        if self.classes_ is None:\n",
    "            self.classes_ = np.unique(y)\n",
    "            self.n_classes = len(self.classes_)\n",
    "        \n",
    "        # Create proper encoding\n",
    "        if self.n_classes == 2:\n",
    "            # Binary classification: use -1, +1 encoding for better numerical properties\n",
    "            y_encoded = np.zeros((len(y), 1))\n",
    "            for i, label in enumerate(y):\n",
    "                y_encoded[i, 0] = 1.0 if label == self.classes_[1] else -1.0\n",
    "        else:\n",
    "            # Multi-class: one-hot encoding\n",
    "            y_encoded = np.zeros((len(y), self.n_classes))\n",
    "            for i, label in enumerate(y):\n",
    "                class_idx = np.where(self.classes_ == label)[0][0]\n",
    "                y_encoded[i, class_idx] = 1.0\n",
    "        \n",
    "        return torch.tensor(y_encoded, dtype=torch.float32, device=self.device)\n",
    "    \n",
    "    def _adaptive_regularization(self, H):\n",
    "        \"\"\"Compute adaptive regularization based on data characteristics.\"\"\"\n",
    "        if not self.adaptive_reg:\n",
    "            return self.regularization\n",
    "        \n",
    "        # Compute condition number estimate\n",
    "        H_norm = torch.norm(H, p='fro')\n",
    "        n_samples = H.shape[0]\n",
    "        \n",
    "        # Adaptive regularization based on data scale and sample size\n",
    "        base_reg = self.regularization\n",
    "        adaptive_factor = max(0.1, min(10.0, H_norm.item() / (n_samples * self.n_hidden)))\n",
    "        \n",
    "        return base_reg * adaptive_factor\n",
    "    \n",
    "    def initial_training(self, X_init, y_init):\n",
    "        \"\"\"Robust initial training phase.\"\"\"\n",
    "        X_init = np.array(X_init, dtype=np.float32)\n",
    "        y_init = np.array(y_init)\n",
    "        \n",
    "        n_samples, n_features = X_init.shape\n",
    "        print(f\"Initial training: {n_samples} samples, {n_features} features, {self.n_hidden} neurons\")\n",
    "        \n",
    "        # Initialize network\n",
    "        if self.input_weights is None:\n",
    "            self._initialize_network(n_features)\n",
    "        \n",
    "        # Compute hidden layer output\n",
    "        H = self._compute_hidden_layer(X_init)\n",
    "        \n",
    "        # Encode targets\n",
    "        T = self._encode_targets(y_init)\n",
    "        \n",
    "        print(f\"Hidden layer shape: {H.shape}, Target shape: {T.shape}\")\n",
    "        \n",
    "        # Compute output weights with proper regularization\n",
    "        try:\n",
    "            # Compute H^T @ H\n",
    "            HtH = torch.mm(H.T, H)  # (n_hidden, n_hidden)\n",
    "            \n",
    "            # Adaptive regularization\n",
    "            reg_param = self._adaptive_regularization(H)\n",
    "            reg_matrix = reg_param * torch.eye(self.n_hidden, device=self.device)\n",
    "            \n",
    "            # Regularized system: (H^T @ H + λI) @ β = H^T @ T\n",
    "            HtH_reg = HtH + reg_matrix\n",
    "            HtT = torch.mm(H.T, T)  # (n_hidden, n_output)\n",
    "            \n",
    "            # Solve using Cholesky decomposition for numerical stability\n",
    "            try:\n",
    "                L = torch.linalg.cholesky(HtH_reg)\n",
    "                # Solve L @ y = HtT\n",
    "                y = torch.linalg.solve_triangular(L, HtT, upper=False)\n",
    "                # Solve L^T @ β = y\n",
    "                self.output_weights = torch.linalg.solve_triangular(L.T, y, upper=True)\n",
    "            except:\n",
    "                # Fallback to direct solve\n",
    "                self.output_weights = torch.linalg.solve(HtH_reg, HtT)\n",
    "            \n",
    "            # Initialize precision matrix for online updates\n",
    "            self.P_matrix = torch.linalg.inv(HtH_reg)\n",
    "            \n",
    "            print(f\"Initial training successful. Regularization: {reg_param:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Initial training failed, using fallback method: {e}\")\n",
    "            # Fallback: use pseudoinverse\n",
    "            H_pinv = torch.linalg.pinv(H)\n",
    "            self.output_weights = torch.mm(H_pinv, T)\n",
    "            self.P_matrix = torch.eye(self.n_hidden, device=self.device) / self.regularization\n",
    "        \n",
    "        self.sample_count = n_samples\n",
    "        self.is_fitted = True\n",
    "        print(f\"Output weights shape: {self.output_weights.shape}\")\n",
    "    \n",
    "    def sequential_update_microbatch(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Micro-batch sequential learning update.\n",
    "        Processes multiple samples together for computational efficiency.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Must perform initial training first\")\n",
    "        \n",
    "        X_batch = np.array(X_batch, dtype=np.float32)\n",
    "        y_batch = np.array(y_batch)\n",
    "        batch_size = len(X_batch)\n",
    "        \n",
    "        # Compute hidden activations for the entire batch\n",
    "        H = self._compute_hidden_layer(X_batch)  # (batch_size, n_hidden)\n",
    "        T = self._encode_targets(y_batch)  # (batch_size, n_output)\n",
    "        \n",
    "        try:\n",
    "            # Batch Sherman-Morrison-Woodbury formula\n",
    "            # For batch updates, we use the matrix inversion lemma:\n",
    "            # (A + UCV)^-1 = A^-1 - A^-1 U (C^-1 + V A^-1 U)^-1 V A^-1\n",
    "            # where A = P_matrix, U = H^T, C = I, V = H\n",
    "            \n",
    "            # P @ H^T (n_hidden, batch_size)\n",
    "            PH_T = torch.mm(self.P_matrix, H.T)\n",
    "            \n",
    "            # H @ P @ H^T + I (batch_size, batch_size)\n",
    "            HPH_T = torch.mm(H, PH_T)\n",
    "            I_batch = torch.eye(batch_size, device=self.device)\n",
    "            inv_term = HPH_T + I_batch\n",
    "            \n",
    "            # Check for numerical stability\n",
    "            try:\n",
    "                inv_HPH_T_I = torch.linalg.inv(inv_term)\n",
    "            except:\n",
    "                # Add small regularization for numerical stability\n",
    "                inv_HPH_T_I = torch.linalg.inv(inv_term + 1e-8 * I_batch)\n",
    "            \n",
    "            # Update precision matrix using Woodbury identity\n",
    "            # P_new = P - P @ H^T @ (H @ P @ H^T + I)^-1 @ H @ P\n",
    "            temp = torch.mm(PH_T, inv_HPH_T_I)\n",
    "            P_update = torch.mm(temp, torch.mm(H, self.P_matrix))\n",
    "            self.P_matrix = self.P_matrix - P_update\n",
    "            \n",
    "            # Compute prediction errors for the batch\n",
    "            current_predictions = torch.mm(H, self.output_weights)  # (batch_size, n_output)\n",
    "            errors = T - current_predictions  # (batch_size, n_output)\n",
    "            \n",
    "            # Update output weights\n",
    "            # β_new = β + P @ H^T @ (H @ P @ H^T + I)^-1 @ errors\n",
    "            weight_update = torch.mm(temp, errors)  # (n_hidden, n_output)\n",
    "            self.output_weights = self.output_weights + weight_update\n",
    "            \n",
    "            self.sample_count += batch_size\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Micro-batch update warning: {e}\")\n",
    "            # Fallback to individual updates\n",
    "            for i in range(batch_size):\n",
    "                try:\n",
    "                    self.sequential_update_single(X_batch[i:i+1], y_batch[i:i+1])\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    def sequential_update_single(self, X_new, y_new):\n",
    "        \"\"\"Single sample sequential learning update (fallback method).\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Must perform initial training first\")\n",
    "        \n",
    "        X_new = np.array(X_new, dtype=np.float32)\n",
    "        y_new = np.array(y_new)\n",
    "        \n",
    "        # Compute hidden activation for this sample\n",
    "        h = self._compute_hidden_layer(X_new)  # (1, n_hidden)\n",
    "        h_T = h.T  # (n_hidden, 1)\n",
    "        \n",
    "        # Encode target\n",
    "        t = self._encode_targets(y_new)  # (1, n_output)\n",
    "        t_T = t.T  # (n_output, 1)\n",
    "        \n",
    "        try:\n",
    "            # Sherman-Morrison formula with numerical safeguards\n",
    "            Ph = torch.mm(self.P_matrix, h_T)  # (n_hidden, 1)\n",
    "            hPh = torch.mm(h, Ph)  # (1, 1)\n",
    "            denominator = 1.0 + hPh.item()\n",
    "            \n",
    "            # Numerical stability check\n",
    "            if abs(denominator) > 1e-8:\n",
    "                # Update precision matrix\n",
    "                P_update = torch.mm(Ph, Ph.T) / denominator\n",
    "                self.P_matrix = self.P_matrix - P_update\n",
    "                \n",
    "                # Compute prediction error\n",
    "                prediction = torch.mm(self.output_weights.T, h_T)  # (n_output, 1)\n",
    "                error = t_T - prediction\n",
    "                \n",
    "                # Update output weights\n",
    "                weight_update = torch.mm(Ph, error.T) / denominator  # (n_hidden, n_output)\n",
    "                self.output_weights = self.output_weights + weight_update\n",
    "                \n",
    "                self.sample_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Single update warning: {e}\")\n",
    "    \n",
    "    def sequential_update(self, X_new, y_new):\n",
    "        \"\"\"\n",
    "        Enhanced sequential update with micro-batching support.\n",
    "        Automatically handles batching based on micro_batch_size.\n",
    "        \"\"\"\n",
    "        X_new = np.array(X_new, dtype=np.float32)\n",
    "        y_new = np.array(y_new)\n",
    "        n_samples = len(X_new)\n",
    "        \n",
    "        # Process in micro-batches\n",
    "        if self.micro_batch_size > 1:\n",
    "            n_batches = (n_samples + self.micro_batch_size - 1) // self.micro_batch_size\n",
    "            \n",
    "            for batch_idx in range(n_batches):\n",
    "                start_idx = batch_idx * self.micro_batch_size\n",
    "                end_idx = min(start_idx + self.micro_batch_size, n_samples)\n",
    "                \n",
    "                X_batch = X_new[start_idx:end_idx]\n",
    "                y_batch = y_new[start_idx:end_idx]\n",
    "                \n",
    "                self.sequential_update_microbatch(X_batch, y_batch)\n",
    "        else:\n",
    "            # Original single-sample processing\n",
    "            for i in range(n_samples):\n",
    "                self.sequential_update_single(X_new[i:i+1], y_new[i:i+1])\n",
    "    \n",
    "    def fit(self, X, y, initial_ratio: float = 0.3):\n",
    "        \"\"\"\n",
    "        Fit the model with optimal initial/sequential split and micro-batching.\n",
    "        \n",
    "        Args:\n",
    "            X: Training features\n",
    "            y: Training labels\n",
    "            initial_ratio: Fraction of data for initial training (0.2-0.5 recommended)\n",
    "        \"\"\"\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        n_samples = len(X)\n",
    "        n_initial = max(self.n_hidden, int(n_samples * initial_ratio))\n",
    "        n_initial = min(n_initial, n_samples)\n",
    "        \n",
    "        print(f\"Training strategy: {n_initial} initial, {n_samples - n_initial} sequential\")\n",
    "        print(f\"Micro-batch size: {self.micro_batch_size}\")\n",
    "        \n",
    "        # Initial training\n",
    "        self.initial_training(X[:n_initial], y[:n_initial])\n",
    "        \n",
    "        # Sequential training with micro-batching\n",
    "        if n_samples > n_initial:\n",
    "            remaining_X = X[n_initial:]\n",
    "            remaining_y = y[n_initial:]\n",
    "            \n",
    "            if self.micro_batch_size > 1:\n",
    "                n_microbatches = (len(remaining_X) + self.micro_batch_size - 1) // self.micro_batch_size\n",
    "                print(f\"Sequential training: {n_microbatches} micro-batches of size ≤{self.micro_batch_size}\")\n",
    "            else:\n",
    "                print(f\"Sequential training: {len(remaining_X)} individual samples\")\n",
    "            \n",
    "            self.sequential_update(remaining_X, remaining_y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted yet\")\n",
    "        \n",
    "        # Compute hidden layer activations\n",
    "        H = self._compute_hidden_layer(X)\n",
    "        \n",
    "        # Compute raw predictions\n",
    "        raw_output = torch.mm(H, self.output_weights)  \n",
    "        \n",
    "        if self.n_classes == 2:\n",
    "            # Binary classification with sigmoid\n",
    "            probs = torch.sigmoid(raw_output).cpu().numpy()\n",
    "            # Return probabilities for both classes\n",
    "            return np.column_stack([1 - probs, probs])\n",
    "        else:\n",
    "            # Multi-class with softmax\n",
    "            probs = torch.softmax(raw_output, dim=1).cpu().numpy()\n",
    "            return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        predicted_indices = np.argmax(probs, axis=1)\n",
    "        return self.classes_[predicted_indices]\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Compute accuracy score.\"\"\"\n",
    "        return accuracy_score(y, self.predict(X))\n",
    "\n",
    "def comprehensive_elm_tuning(file_path, test_configurations, device='auto'):\n",
    "    \"\"\"\n",
    "    Comprehensive ELM tuning with micro-batching support.\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(file_path)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"COMPREHENSIVE OS-ELM TUNING FOR {filename}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Load and preprocess data\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filename}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Data preprocessing\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        print(\"Handling missing values...\")\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())\n",
    "    \n",
    "    # Prepare features and targets\n",
    "    X = df.iloc[:, :-1].values.astype(np.float32)\n",
    "    y = df.iloc[:, -1].values\n",
    "    \n",
    "    print(f\"Features: {X.shape[1]}, Samples: {X.shape[0]}\")\n",
    "    print(f\"Classes: {np.unique(y)}, Distribution: {np.bincount(y.astype(int))}\")\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train_scaled)}, Test samples: {len(X_test_scaled)}\")\n",
    "    \n",
    "    # Run experiments\n",
    "    results = []\n",
    "    total_configs = len(test_configurations)\n",
    "    \n",
    "    for config_idx, config in enumerate(test_configurations):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Configuration {config_idx + 1}/{total_configs}\")\n",
    "        print(f\"Config: {config}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            # Create model\n",
    "            model = RobustOSELM(\n",
    "                n_hidden=config['n_hidden'],\n",
    "                activation=config['activation'],\n",
    "                regularization=config['regularization'],\n",
    "                device=device,\n",
    "                random_state=42,\n",
    "                adaptive_reg=config.get('adaptive_reg', True),\n",
    "                micro_batch_size=config.get('micro_batch_size', 1)\n",
    "            )\n",
    "            \n",
    "            # Training\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train_scaled, y_train, initial_ratio=config.get('initial_ratio', 0.3))\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluation\n",
    "            train_accuracy = model.score(X_train_scaled, y_train)\n",
    "            test_accuracy = model.score(X_test_scaled, y_test)\n",
    "            \n",
    "            result = {\n",
    "                'n_hidden': config['n_hidden'],\n",
    "                'activation': config['activation'],\n",
    "                'regularization': config['regularization'],\n",
    "                'initial_ratio': config.get('initial_ratio', 0.3),\n",
    "                'adaptive_reg': config.get('adaptive_reg', True),\n",
    "                'micro_batch_size': config.get('micro_batch_size', 1),\n",
    "                'train_accuracy': train_accuracy,\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'training_time': training_time,\n",
    "                'overfitting': train_accuracy - test_accuracy\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"Results:\")\n",
    "            print(f\"  Train Accuracy: {train_accuracy*100:.2f}%\")\n",
    "            print(f\"  Test Accuracy:  {test_accuracy*100:.2f}%\")\n",
    "            print(f\"  Training Time:  {training_time:.3f}s\")\n",
    "            print(f\"  Overfitting:    {(train_accuracy - test_accuracy)*100:.2f}%\")\n",
    "            \n",
    "            \n",
    "            if device != 'cpu':\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Configuration failed: {e}\")\n",
    "            results.append({\n",
    "                'n_hidden': config['n_hidden'],\n",
    "                'activation': config['activation'],\n",
    "                'regularization': config['regularization'],\n",
    "                'initial_ratio': config.get('initial_ratio', 0.3),\n",
    "                'adaptive_reg': config.get('adaptive_reg', True),\n",
    "                'micro_batch_size': config.get('micro_batch_size', 1),\n",
    "                'train_accuracy': 0.0,\n",
    "                'test_accuracy': 0.0,\n",
    "                'training_time': float('inf'),\n",
    "                'overfitting': float('inf')\n",
    "            })\n",
    "    \n",
    "    # Analysis and visualization\n",
    "    analyze_results(results, filename)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_results(results, filename):\n",
    "    \"\"\"Comprehensive analysis of tuning results.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    valid_results = df_results[df_results['test_accuracy'] > 0].copy()\n",
    "    \n",
    "    if valid_results.empty:\n",
    "        print(\"No valid results found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"ANALYSIS SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Best configuration\n",
    "    best_idx = valid_results['test_accuracy'].idxmax()\n",
    "    best_config = valid_results.iloc[best_idx]\n",
    "    \n",
    "    print(f\"Best Configuration:\")\n",
    "    print(f\"  Hidden Neurons: {best_config['n_hidden']}\")\n",
    "    print(f\"  Activation: {best_config['activation']}\")\n",
    "    print(f\"  Regularization: {best_config['regularization']:.4f}\")\n",
    "    print(f\"  Micro-batch Size: {best_config['micro_batch_size']}\")\n",
    "    print(f\"  Test Accuracy: {best_config['test_accuracy']*100:.2f}%\")\n",
    "    print(f\"  Training Time: {best_config['training_time']:.3f}s\")\n",
    "    \n",
    "    \n",
    "    if 'micro_batch_size' in valid_results.columns:\n",
    "        print(f\"\\nPerformance by Micro-batch Size:\")\n",
    "        for batch_size in sorted(valid_results['micro_batch_size'].unique()):\n",
    "            subset = valid_results[valid_results['micro_batch_size'] == batch_size]\n",
    "            avg_acc = subset['test_accuracy'].mean()\n",
    "            avg_time = subset['training_time'].mean()\n",
    "            print(f\"  Batch size {batch_size}: {avg_acc*100:.2f}% accuracy, {avg_time:.3f}s avg time\")\n",
    "    \n",
    "    # Statistics by activation function\n",
    "    print(f\"\\nPerformance by Activation Function:\")\n",
    "    for activation in valid_results['activation'].unique():\n",
    "        subset = valid_results[valid_results['activation'] == activation]\n",
    "        avg_acc = subset['test_accuracy'].mean()\n",
    "        std_acc = subset['test_accuracy'].std()\n",
    "        print(f\"  {activation}: {avg_acc*100:.2f}% ± {std_acc*100:.2f}%\")\n",
    "\n",
    "# Example usage with micro-batching configurations\n",
    "if __name__ == \"__main__\":\n",
    "    # Enhanced configurations with micro-batching\n",
    "    test_configurations = [\n",
    "        # Compare micro-batch sizes with moderate hidden neurons\n",
    "        {'n_hidden': 2500, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 1},\n",
    "        {'n_hidden': 2500, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 5},\n",
    "        {'n_hidden': 2500, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 10},\n",
    "        {'n_hidden': 2500, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 20},\n",
    "        \n",
    "        # Test larger hidden neurons with micro-batching\n",
    "        {'n_hidden': 5000, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 10},\n",
    "        {'n_hidden': 7500, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 10},\n",
    "        {'n_hidden': 10000, 'activation': 'gelu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 10},\n",
    "        \n",
    "        # Compare activations with micro-batching\n",
    "        {'n_hidden': 5000, 'activation': 'tanh', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 10},\n",
    "        {'n_hidden': 5000, 'activation': 'relu', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 10},\n",
    "        {'n_hidden': 5000, 'activation': 'swish', 'regularization': 0.1, 'initial_ratio': 0.1, 'micro_batch_size': 10},\n",
    "    ]\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    DATASET_PATH = '../Datasets'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        dataset_files = [os.path.join(DATASET_PATH, f) \n",
    "                        for f in os.listdir(DATASET_PATH) \n",
    "                        if f.endswith('.csv')]\n",
    "        \n",
    "        print(f\"Found {len(dataset_files)} dataset files\")\n",
    "        \n",
    "        all_results = {}\n",
    "        for file in dataset_files:\n",
    "            results = comprehensive_elm_tuning(file, test_configurations, device)\n",
    "            all_results[file] = results\n",
    "        \n",
    "        # Overall summary\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"OVERALL SUMMARY ACROSS ALL DATASETS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        for file, results in all_results.items():\n",
    "            if results:\n",
    "                df_res = pd.DataFrame(results)\n",
    "                valid_res = df_res[df_res['test_accuracy'] > 0]\n",
    "                if not valid_res.empty:\n",
    "                    best_acc = valid_res['test_accuracy'].max()\n",
    "                    filename = os.path.basename(file)\n",
    "                    print(f\"{filename}: Best accuracy = {best_acc*100:.2f}%\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Dataset path {DATASET_PATH} not found!\")\n",
    "    \n",
    "    print(\"\\nComprehensive tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a55ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ae455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
